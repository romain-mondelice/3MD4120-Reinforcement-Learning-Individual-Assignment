{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Flappy Bird\n",
    "**Romain Mondelice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to apply reinforcement learning methods to a\n",
    "simple game called Text Flappy Bird (TFB). The game is a variation to the\n",
    "well know Flappy Bird in which the player is made with a simple unit-element\n",
    "character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import text_flappy_bird_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Monte Carlo based agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMonteCarloAgent:\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(self.zero_action_value)\n",
    "        self.C = defaultdict(self.zero_action_value)\n",
    "        self.target_policy = defaultdict(int)\n",
    "\n",
    "    def zero_action_value(self):\n",
    "        return np.zeros(self.env.action_space.n)\n",
    "        \n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Convert state to a string representation.\n",
    "            str_state = str(state)\n",
    "\n",
    "            if str_state in policy:\n",
    "                action_probs = policy[str_state]\n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            else:\n",
    "                # Fallback if the state is not in the policy, use uniform random selection\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _, info = self.env.step(action)\n",
    "            episode.append((str_state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def get_probs(self, Q_s, epsilon, nA):\n",
    "        \"\"\"Obtains the policy for a given state\"\"\"\n",
    "        policy_s = np.ones(nA) * epsilon / nA\n",
    "        best_a = np.argmax(Q_s)\n",
    "        policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "        return policy_s\n",
    "    \n",
    "    def update_Q(self, episode):\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            self.C[state][action] += W\n",
    "            self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\n",
    "            self.target_policy[state] = np.argmax(self.Q[state])\n",
    "            \n",
    "            if action != self.target_policy[state]:\n",
    "                break\n",
    "            W = W * 1./self.get_probs(self.Q[state], self.epsilon, self.env.action_space.n)[action]\n",
    "            \n",
    "    def train(self, num_episodes):\n",
    "        for i_episode in range(1, num_episodes + 1):\n",
    "            episode = self.generate_episode(policy=self.create_behavior_policy(self.Q))\n",
    "            self.update_Q(episode)\n",
    "    \n",
    "    def create_behavior_policy(self, Q):\n",
    "        \"\"\"Creates a behavior policy using Îµ-greedy approach based on Q.\"\"\"\n",
    "        behavior_policy = {}\n",
    "        for state, actions in Q.items():\n",
    "            behavior_policy[state] = self.get_probs(actions, self.epsilon, self.env.action_space.n)\n",
    "        return behavior_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_monte_carlo_agent(env_str, episodes=1000, gamma=0.9, epsilon=0.1):\n",
    "    env = gym.make(env_str, height=15, width=20, pipe_gap=4)\n",
    "    agent = OffPolicyMonteCarloAgent(env, gamma=gamma, epsilon=epsilon)\n",
    "    \n",
    "    episode_rewards = []  # List to store total reward from each episode\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc=\"Training process\"):\n",
    "        # Create the behavior policy from current Q\n",
    "        behavior_policy = agent.create_behavior_policy(agent.Q)\n",
    "        # Generate an episode using the behavior policy\n",
    "        episode = agent.generate_episode(behavior_policy)\n",
    "        # Update Q-values based on the episode\n",
    "        agent.update_Q(episode)\n",
    "        \n",
    "        # Calculate total reward for the episode and store it\n",
    "        total_reward = sum([reward for (_, _, reward) in episode])\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return agent, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, episodes=100, reward_threshold=10000):\n",
    "    total_rewards = 0\n",
    "    episode_scores = []  # To store the score of each episode\n",
    "\n",
    "    for episode_num in tqdm(range(1, episodes + 1), desc=\"Testing episodes\"):\n",
    "        state = agent.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Convert state to a string representation for consistency.\n",
    "            str_state = str(state)\n",
    "            \n",
    "            # Use the target_policy for action selection if this state has been seen.\n",
    "            # Otherwise, select a random action.\n",
    "            if str_state in agent.target_policy:\n",
    "                action = agent.target_policy[str_state]\n",
    "            else:\n",
    "                action = agent.env.action_space.sample()\n",
    "\n",
    "            state, reward, done, _, info = agent.env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Check if the reward threshold for this episode has been exceeded\n",
    "            if episode_reward > reward_threshold:\n",
    "                break\n",
    "\n",
    "        # Episode is done or threshold exceeded, append its total reward to episode_scores\n",
    "        episode_scores.append(episode_reward)\n",
    "        total_rewards += episode_reward\n",
    "    \n",
    "    avg_reward = total_rewards / episodes\n",
    "    print(\"Total reward across all episodes: \", total_rewards)\n",
    "    print(f\"Average Reward over {episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Flappy Bird Screen env**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Sensitivity analysis*\n",
    "Conducted over 1000 episodes, the goal is to check the sensitivity of different hyperparameters to be able to find optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8, 0.85, 0.9, 0.95, 1.0]  # Gamma values\n",
    "epsilons = [0.1, 0.2, 0.3, 0.4, 0.5]  # Epsilon values\n",
    "performance_metrics = np.zeros((len(gammas), len(epsilons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gamma in enumerate(gammas):\n",
    "    for j, epsilon in enumerate(epsilons):\n",
    "        trained_agent, episode_rewards = train_monte_carlo_agent('TextFlappyBird-screen-v0', episodes=5000, gamma=gamma, epsilon=epsilon)\n",
    "        total_reward = test_agent(trained_agent, episodes=100)\n",
    "        performance_metrics[i, j] = total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(performance_metrics, xticklabels=epsilons, yticklabels=gammas, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Gamma')\n",
    "plt.title('Agent Performance for Different Gamma and Epsilon Values')\n",
    "\n",
    "plot_file_path = \"../reports/figures/sensitivity_analysis_mc_screen.png\"\n",
    "plt.savefig(plot_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Best params training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyper param epsilon and gamma\n",
    "# Find the position of the maximum value in the performance metrics matrix\n",
    "max_value_index = np.unravel_index(performance_metrics.argmax(), performance_metrics.shape)\n",
    "\n",
    "# Retrieve the corresponding epsilon and gamma values\n",
    "best_epsilon = epsilons[max_value_index[1]]  # Column index for epsilon\n",
    "best_gamma = gammas[max_value_index[0]]  # Row index for gamma\n",
    "\n",
    "# Train\n",
    "trained_agent, episode_rewards = train_monte_carlo_agent('TextFlappyBird-screen-v0', episodes=25000, gamma=best_gamma, epsilon=best_epsilon)\n",
    "\n",
    "# Save agent and episode reward history\n",
    "episode_rewards_df = pd.DataFrame(episode_rewards, columns=[\"Reward\"])\n",
    "episode_rewards_df.to_csv('../saves/episode_rewards_mc_screen.csv', index_label=\"Episode\")\n",
    "\n",
    "with open('../agents/mc-agent-screen.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling mean\n",
    "rolling_mean = episode_rewards_df['Reward'].rolling(window=500).mean()\n",
    "\n",
    "# Plot the rolling mean of reward evolution during training\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rolling_mean, label='500 Episode Rolling Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Evolution During Training (500 Episode Rolling Mean)')\n",
    "plt.legend()\n",
    "plt.savefig('../reports/figures/reward_evolution_rolling_mean_mc_screen.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Flappy Bird Dist Env**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Sensitivity analysis*\n",
    "Conducted over 1000 episodes, the goal is to check the sensitivity of different hyperparameters to be able to find optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8, 0.85, 0.9, 0.95, 1.0]  # Gamma values\n",
    "epsilons = [0.1, 0.2, 0.3, 0.4, 0.5]  # Epsilon values\n",
    "performance_metrics = np.zeros((len(gammas), len(epsilons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gamma in enumerate(gammas):\n",
    "    for j, epsilon in enumerate(epsilons):\n",
    "        trained_agent, episode_rewards = train_monte_carlo_agent('TextFlappyBird-v0', episodes=5000, gamma=gamma, epsilon=epsilon)\n",
    "        total_reward = test_agent(trained_agent, episodes=100)\n",
    "        performance_metrics[i, j] = total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(performance_metrics, xticklabels=epsilons, yticklabels=gammas, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Gamma')\n",
    "plt.title('Agent Performance for Different Gamma and Epsilon Values')\n",
    "\n",
    "plot_file_path = \"../reports/figures/sensitivity_analysis_mc_dist.png\"\n",
    "plt.savefig(plot_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Best params training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyper param epsilon and gamma\n",
    "# Find the position of the maximum value in the performance metrics matrix\n",
    "max_value_index = np.unravel_index(performance_metrics.argmax(), performance_metrics.shape)\n",
    "\n",
    "# Retrieve the corresponding epsilon and gamma values\n",
    "best_epsilon = epsilons[max_value_index[1]]  # Column index for epsilon\n",
    "best_gamma = gammas[max_value_index[0]]  # Row index for gamma\n",
    "\n",
    "# Train\n",
    "trained_agent, episode_rewards = train_monte_carlo_agent('TextFlappyBird-v0', episodes=25000, gamma=best_gamma, epsilon=best_epsilon)\n",
    "\n",
    "# Save agent and episode reward history\n",
    "episode_rewards_df = pd.DataFrame(episode_rewards, columns=[\"Reward\"])\n",
    "episode_rewards_df.to_csv('../saves/episode_rewards_mc_dist.csv', index_label=\"Episode\")\n",
    "\n",
    "with open('../agents/mc-agent-dist.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling mean\n",
    "rolling_mean = episode_rewards_df['Reward'].rolling(window=500).mean()\n",
    "\n",
    "# Plot the rolling mean of reward evolution during training\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rolling_mean, label='500 Episode Rolling Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Evolution During Training (500 Episode Rolling Mean)')\n",
    "plt.legend()\n",
    "plt.savefig('../reports/figures/reward_evolution_rolling_mean_mc_dist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test of trained agent on different level configuration (check overfitting)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Test on screen agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "with open('../agents/mc-agent-screen.pkl', 'rb') as f:\n",
    "    trained_agent_screen = pickle.load(f)\n",
    "\n",
    "# Define the configurations to test\n",
    "configurations = [\n",
    "    {'height': 15, 'width': 20, 'pipe_gap': 4},\n",
    "    {'height': 20, 'width': 25, 'pipe_gap': 7},\n",
    "    {'height': 25, 'width': 33, 'pipe_gap': 10}\n",
    "]\n",
    "\n",
    "# Placeholder for average rewards\n",
    "average_rewards = []\n",
    "\n",
    "# Test the agent on different configurations and collect average rewards\n",
    "for config in configurations:\n",
    "    print(f\"Testing on configuration: {config}\")\n",
    "    env = gym.make('TextFlappyBird-screen-v0', height=config['height'], width=config['width'], pipe_gap=config['pipe_gap'])\n",
    "    trained_agent_screen.env = env\n",
    "    avg_reward = test_agent(trained_agent_screen, episodes=100, reward_threshold=10000)\n",
    "    average_rewards.append(avg_reward)\n",
    "    print(f\"Average reward for configuration {config}: {avg_reward}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the x-axis\n",
    "config_labels = [f\"Height: {config['height']}, Width: {config['width']}, Pipe Gap: {config['pipe_gap']}\" for config in configurations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(config_labels, average_rewards)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward for Different Configurations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/average_reward_histogram_mc_screen.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Test on dist agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "with open('../agents/mc-agent-dist.pkl', 'rb') as f:\n",
    "    trained_agent_dist = pickle.load(f)\n",
    "\n",
    "# Define the configurations to test\n",
    "configurations = [\n",
    "    {'height': 15, 'width': 20, 'pipe_gap': 4},\n",
    "    {'height': 20, 'width': 25, 'pipe_gap': 7},\n",
    "    {'height': 25, 'width': 33, 'pipe_gap': 10}\n",
    "]\n",
    "\n",
    "# Placeholder for average rewards\n",
    "average_rewards = []\n",
    "\n",
    "# Test the agent on different configurations and collect average rewards\n",
    "for config in configurations:\n",
    "    print(f\"Testing on configuration: {config}\")\n",
    "    env = gym.make('TextFlappyBird-v0', height=config['height'], width=config['width'], pipe_gap=config['pipe_gap'])\n",
    "    trained_agent_dist.env = env\n",
    "    avg_reward = test_agent(trained_agent_dist, episodes=100, reward_threshold=10000)\n",
    "    average_rewards.append(avg_reward)\n",
    "    print(f\"Average reward for configuration {config}: {avg_reward}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the x-axis\n",
    "config_labels = [f\"Height: {config['height']}, Width: {config['width']}, Pipe Gap: {config['pipe_gap']}\" for config in configurations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(config_labels, average_rewards)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward for Different Configurations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/average_reward_histogram_mc_dist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sarsa based agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLambdaAgent:\n",
    "    def __init__(self, env, gamma=0.9, lambda_=0.9, epsilon=0.1, alpha=0.5):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.Q = defaultdict(self.zero_action_value)\n",
    "        self.E = defaultdict(self.zero_action_value)\n",
    "\n",
    "    def zero_action_value(self):\n",
    "        return np.zeros(self.env.action_space.n)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        delta = reward + self.gamma * self.Q[next_state][next_action] * (not done) - self.Q[state][action]\n",
    "        self.E[state][action] += 1\n",
    "\n",
    "        for s, values in self.Q.items():\n",
    "            for a in range(len(values)):\n",
    "                self.Q[s][a] += self.alpha * delta * self.E[s][a]\n",
    "                self.E[s][a] *= self.gamma * self.lambda_\n",
    "\n",
    "    def reset_eligibility_traces(self):\n",
    "        for s in self.E:\n",
    "            for a in range(len(self.E[s])):\n",
    "                self.E[s][a] = 0\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for i_episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            action = self.choose_action(str(state))\n",
    "            self.reset_eligibility_traces()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                next_action = self.choose_action(str(next_state))\n",
    "                self.update(str(state), action, reward, str(next_state), next_action, done)\n",
    "                state = next_state\n",
    "                action = next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa_lambda_agent(env_str, episodes=1000, gamma=0.9, lambda_=0.9, epsilon=0.1, alpha=0.5):\n",
    "    env = gym.make(env_str, height=15, width=20, pipe_gap=4)\n",
    "    agent = SarsaLambdaAgent(env, gamma=gamma, lambda_=lambda_, epsilon=epsilon, alpha=alpha)\n",
    "    \n",
    "    episode_rewards = []\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc=\"Training process\"):\n",
    "        state = env.reset()\n",
    "        action = agent.choose_action(str(state))\n",
    "        agent.reset_eligibility_traces()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_action = agent.choose_action(str(next_state))\n",
    "            \n",
    "            # Update Q-values based on the transition\n",
    "            agent.update(str(state), action, reward, str(next_state), next_action, done)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return agent, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sarsa_lambda_agent(agent, episodes=100, reward_threshold=10000):\n",
    "    total_rewards = 0\n",
    "    episode_scores = []  # To store the score of each episode\n",
    "\n",
    "    for episode_num in tqdm(range(1, episodes + 1), desc=\"Testing episodes\"):\n",
    "        state = agent.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(str(state))  # Directly choose action from agent's policy\n",
    "            \n",
    "            next_state, reward, done, _, info = agent.env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state  # Update the state\n",
    "\n",
    "            # Check if the reward threshold for this episode has been exceeded\n",
    "            if episode_reward > reward_threshold:\n",
    "                break\n",
    "\n",
    "        # Episode is done or threshold exceeded, append its total reward to episode_scores\n",
    "        episode_scores.append(episode_reward)\n",
    "        total_rewards += episode_reward\n",
    "    \n",
    "    avg_reward = total_rewards / episodes\n",
    "    print(\"Total reward across all episodes: \", total_rewards)\n",
    "    print(f\"Average Reward over {episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Flappy Bird Screen env**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Sensitivity analysis*\n",
    "Conducted over 1000 episodes, the goal is to check the sensitivity of different hyperparameters to be able to find optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8, 1.0]\n",
    "epsilons = [0.1, 0.5]\n",
    "lambdas = [0.8, 1.0]\n",
    "alphas = [0.1, 0.5]\n",
    "\n",
    "# Create a list of all possible combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(gammas, epsilons, lambdas, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the performance results\n",
    "performance_metrics = []\n",
    "\n",
    "for gamma, epsilon, lambda_, alpha in param_combinations:\n",
    "    trained_agent, episode_rewards = train_sarsa_lambda_agent('TextFlappyBird-screen-v0', episodes=5000, gamma=gamma, epsilon=epsilon, lambda_=lambda_, alpha=alpha)\n",
    "    total_reward = test_sarsa_lambda_agent(trained_agent, episodes=100)\n",
    "    performance_metrics.append([gamma, epsilon, lambda_, alpha, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(performance_metrics, columns=['Gamma', 'Epsilon', 'Lambda', 'Alpha', 'Total Reward'])\n",
    "\n",
    "# Pivot the DataFrame to create a multi-index for the heatmap\n",
    "pivot_df = df.pivot_table(index=['Gamma', 'Lambda'], columns=['Epsilon', 'Alpha'], values='Total Reward')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_df, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel('Epsilon - Alpha')\n",
    "plt.ylabel('Gamma - Lambda')\n",
    "plt.title('Agent Performance for Different Hyperparameter Combinations')\n",
    "plot_file_path = \"../reports/figures/sensitivity_analysis_sarsa_screen.png\"\n",
    "plt.savefig(plot_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Best params training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyper param epsilon and gamma\n",
    "# Find the row with the maximum total reward\n",
    "best_params_row = df.loc[df['Total Reward'].idxmax()]\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_gamma = best_params_row['Gamma']\n",
    "best_epsilon = best_params_row['Epsilon']\n",
    "best_lambda = best_params_row['Lambda']\n",
    "best_alpha = best_params_row['Alpha']\n",
    "\n",
    "# Train\n",
    "trained_agent, episode_rewards = train_sarsa_lambda_agent('TextFlappyBird-screen-v0', episodes=25000, gamma=best_gamma, epsilon=best_epsilon, lambda_=best_lambda, alpha=best_alpha)\n",
    "\n",
    "# Save agent and episode reward history\n",
    "episode_rewards_df = pd.DataFrame(episode_rewards, columns=[\"Reward\"])\n",
    "episode_rewards_df.to_csv('../saves/episode_rewards_sarsa_screen.csv', index_label=\"Episode\")\n",
    "\n",
    "with open('../agents/sarsa-agent-screen.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling mean\n",
    "rolling_mean = episode_rewards_df['Reward'].rolling(window=500).mean()\n",
    "\n",
    "# Plot the rolling mean of reward evolution during training\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rolling_mean, label='500 Episode Rolling Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Evolution During Training (500 Episode Rolling Mean)')\n",
    "plt.legend()\n",
    "plt.savefig('../reports/figures/reward_evolution_rolling_mean_sarsa_screen.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Flappy Bird dist env**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Sensitivity analysis*\n",
    "Conducted over 1000 episodes, the goal is to check the sensitivity of different hyperparameters to be able to find optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8, 1.0]\n",
    "epsilons = [0.1, 0.5]\n",
    "lambdas = [0.8, 1.0]\n",
    "alphas = [0.1, 0.5]\n",
    "\n",
    "# Create a list of all possible combinations of hyperparameters\n",
    "param_combinations = list(itertools.product(gammas, epsilons, lambdas, alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the performance results\n",
    "performance_metrics = []\n",
    "\n",
    "for gamma, epsilon, lambda_, alpha in param_combinations:\n",
    "    trained_agent, episode_rewards = train_sarsa_lambda_agent('TextFlappyBird-v0', episodes=5000, gamma=gamma, epsilon=epsilon, lambda_=lambda_, alpha=alpha)\n",
    "    total_reward = test_sarsa_lambda_agent(trained_agent, episodes=100)\n",
    "    performance_metrics.append([gamma, epsilon, lambda_, alpha, total_reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(performance_metrics, columns=['Gamma', 'Epsilon', 'Lambda', 'Alpha', 'Total Reward'])\n",
    "\n",
    "# Pivot the DataFrame to create a multi-index for the heatmap\n",
    "pivot_df = df.pivot_table(index=['Gamma', 'Lambda'], columns=['Epsilon', 'Alpha'], values='Total Reward')\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_df, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel('Epsilon - Alpha')\n",
    "plt.ylabel('Gamma - Lambda')\n",
    "plt.title('Agent Performance for Different Hyperparameter Combinations')\n",
    "plot_file_path = \"../reports/figures/sensitivity_analysis_sarsa_dist.png\"\n",
    "plt.savefig(plot_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyper param epsilon and gamma\n",
    "# Find the row with the maximum total reward\n",
    "best_params_row = df.loc[df['Total Reward'].idxmax()]\n",
    "\n",
    "# Extract the best hyperparameters\n",
    "best_gamma = best_params_row['Gamma']\n",
    "best_epsilon = best_params_row['Epsilon']\n",
    "best_lambda = best_params_row['Lambda']\n",
    "best_alpha = best_params_row['Alpha']\n",
    "\n",
    "# Train\n",
    "trained_agent, episode_rewards = train_sarsa_lambda_agent('TextFlappyBird-v0', episodes=25000, gamma=best_gamma, epsilon=best_epsilon, lambda_=best_lambda, alpha=best_alpha)\n",
    "\n",
    "# Save agent and episode reward history\n",
    "episode_rewards_df = pd.DataFrame(episode_rewards, columns=[\"Reward\"])\n",
    "episode_rewards_df.to_csv('../saves/episode_rewards_sarsa_dist.csv', index_label=\"Episode\")\n",
    "\n",
    "with open('../agents/sarsa-agent-dist.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling mean\n",
    "rolling_mean = episode_rewards_df['Reward'].rolling(window=500).mean()\n",
    "\n",
    "# Plot the rolling mean of reward evolution during training\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=rolling_mean, label='500 Episode Rolling Mean')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward Evolution During Training (500 Episode Rolling Mean)')\n",
    "plt.legend()\n",
    "plt.savefig('../reports/figures/reward_evolution_rolling_mean_sarsa_dist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test of trained agent on different level configuration (check overfitting)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Test on screen agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "with open('../agents/sarsa-agent-screen.pkl', 'rb') as f:\n",
    "    trained_agent_screen = pickle.load(f)\n",
    "\n",
    "# Define the configurations to test\n",
    "configurations = [\n",
    "    {'height': 15, 'width': 20, 'pipe_gap': 4},\n",
    "    {'height': 20, 'width': 25, 'pipe_gap': 7},\n",
    "    {'height': 25, 'width': 33, 'pipe_gap': 10}\n",
    "]\n",
    "\n",
    "# Placeholder for average rewards\n",
    "average_rewards = []\n",
    "\n",
    "# Test the agent on different configurations and collect average rewards\n",
    "for config in configurations:\n",
    "    print(f\"Testing on configuration: {config}\")\n",
    "    env = gym.make('TextFlappyBird-screen-v0', height=config['height'], width=config['width'], pipe_gap=config['pipe_gap'])\n",
    "    trained_agent_screen.env = env\n",
    "    avg_reward = test_sarsa_lambda_agent(trained_agent_screen, episodes=100, reward_threshold=10000)\n",
    "    average_rewards.append(avg_reward)\n",
    "    print(f\"Average reward for configuration {config}: {avg_reward}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the x-axis\n",
    "config_labels = [f\"Height: {config['height']}, Width: {config['width']}, Pipe Gap: {config['pipe_gap']}\" for config in configurations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(config_labels, average_rewards)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward for Different Configurations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/average_reward_histogram_sarsa_screen.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Test on dist agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "with open('../agents/sarsa-agent-dist.pkl', 'rb') as f:\n",
    "    trained_agent_dist = pickle.load(f)\n",
    "\n",
    "# Define the configurations to test\n",
    "configurations = [\n",
    "    {'height': 15, 'width': 20, 'pipe_gap': 4},\n",
    "    {'height': 20, 'width': 25, 'pipe_gap': 7},\n",
    "    {'height': 25, 'width': 33, 'pipe_gap': 10}\n",
    "]\n",
    "\n",
    "# Placeholder for average rewards\n",
    "average_rewards = []\n",
    "\n",
    "# Test the agent on different configurations and collect average rewards\n",
    "for config in configurations:\n",
    "    print(f\"Testing on configuration: {config}\")\n",
    "    env = gym.make('TextFlappyBird-v0', height=config['height'], width=config['width'], pipe_gap=config['pipe_gap'])\n",
    "    trained_agent_dist.env = env\n",
    "    avg_reward = test_sarsa_lambda_agent(trained_agent_dist, episodes=100, reward_threshold=10000)\n",
    "    average_rewards.append(avg_reward)\n",
    "    print(f\"Average reward for configuration {config}: {avg_reward}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the x-axis\n",
    "config_labels = [f\"Height: {config['height']}, Width: {config['width']}, Pipe Gap: {config['pipe_gap']}\" for config in configurations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(config_labels, average_rewards)\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward for Different Configurations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/average_reward_histogram_sarsa_dist.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EC-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
