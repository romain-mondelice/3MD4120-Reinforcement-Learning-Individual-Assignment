{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Flappy Bird\n",
    "*Romain Mondelice*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to apply reinforcement learning methods to a\n",
    "simple game called Text Flappy Bird (TFB). The game is a variation to the\n",
    "well know Flappy Bird in which the player is made with a simple unit-element\n",
    "character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import text_flappy_bird_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate environment\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "while True:\n",
    "        # Select next action\n",
    "        action = env.action_space.sample()  # for an agent, action = agent.policy(observation)\n",
    "\n",
    "        # Appy action and return new observation of the environment\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Render the game\n",
    "        os.system(\"clear\")\n",
    "        sys.stdout.write(env.render())\n",
    "        time.sleep(0.2) # FPS\n",
    "\n",
    "        # If player is dead break\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMonteCarloAgent:\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(self.zero_action_value)\n",
    "        self.C = defaultdict(self.zero_action_value)\n",
    "        self.target_policy = defaultdict(int)\n",
    "\n",
    "    def zero_action_value(self):\n",
    "        return np.zeros(self.env.action_space.n)\n",
    "        \n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Convert state to a string representation.\n",
    "            str_state = str(state)\n",
    "\n",
    "            if str_state in policy:\n",
    "                action_probs = policy[str_state]\n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            else:\n",
    "                # Fallback if the state is not in the policy, use uniform random selection\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _, info = self.env.step(action)\n",
    "            episode.append((str_state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def get_probs(self, Q_s, epsilon, nA):\n",
    "        \"\"\"Obtains the policy for a given state\"\"\"\n",
    "        policy_s = np.ones(nA) * epsilon / nA\n",
    "        best_a = np.argmax(Q_s)\n",
    "        policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "        return policy_s\n",
    "    \n",
    "    def update_Q(self, episode):\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            self.C[state][action] += W\n",
    "            self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\n",
    "            self.target_policy[state] = np.argmax(self.Q[state])\n",
    "            \n",
    "            if action != self.target_policy[state]:\n",
    "                break\n",
    "            W = W * 1./self.get_probs(self.Q[state], self.epsilon, self.env.action_space.n)[action]\n",
    "            \n",
    "    def train(self, num_episodes):\n",
    "        for i_episode in range(1, num_episodes + 1):\n",
    "            episode = self.generate_episode(policy=self.create_behavior_policy(self.Q))\n",
    "            self.update_Q(episode)\n",
    "    \n",
    "    def create_behavior_policy(self, Q):\n",
    "        \"\"\"Creates a behavior policy using Îµ-greedy approach based on Q.\"\"\"\n",
    "        behavior_policy = {}\n",
    "        for state, actions in Q.items():\n",
    "            behavior_policy[state] = self.get_probs(actions, self.epsilon, self.env.action_space.n)\n",
    "        return behavior_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_monte_carlo_agent(env_str, episodes=1000, gamma=0.9, epsilon=0.1):\n",
    "    env = gym.make(env_str, height=15, width=20, pipe_gap=4)\n",
    "    agent = OffPolicyMonteCarloAgent(env, gamma=gamma, epsilon=epsilon)\n",
    "    \n",
    "    episode_rewards = []  # List to store total reward from each episode\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc=\"Training process\"):\n",
    "        # Create the behavior policy from current Q\n",
    "        behavior_policy = agent.create_behavior_policy(agent.Q)\n",
    "        # Generate an episode using the behavior policy\n",
    "        episode = agent.generate_episode(behavior_policy)\n",
    "        # Update Q-values based on the episode\n",
    "        agent.update_Q(episode)\n",
    "        \n",
    "        # Calculate total reward for the episode and store it\n",
    "        total_reward = sum([reward for (_, _, reward) in episode])\n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return agent, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, episodes=100, reward_threshold=10000):\n",
    "    total_rewards = 0\n",
    "    episode_scores = []  # To store the score of each episode\n",
    "\n",
    "    for episode_num in tqdm(range(1, episodes + 1), desc=\"Testing episodes\"):\n",
    "        state = agent.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Convert state to a string representation for consistency.\n",
    "            str_state = str(state)\n",
    "            \n",
    "            # Use the target_policy for action selection if this state has been seen.\n",
    "            # Otherwise, select a random action.\n",
    "            if str_state in agent.target_policy:\n",
    "                action = agent.target_policy[str_state]\n",
    "            else:\n",
    "                action = agent.env.action_space.sample()\n",
    "\n",
    "            state, reward, done, _, info = agent.env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Check if the reward threshold for this episode has been exceeded\n",
    "            if episode_reward > reward_threshold:\n",
    "                break\n",
    "\n",
    "        # Episode is done or threshold exceeded, append its total reward to episode_scores\n",
    "        episode_scores.append(episode_reward)\n",
    "        total_rewards += episode_reward\n",
    "    \n",
    "    avg_reward = total_rewards / episodes\n",
    "    print(\"Total reward across all episodes: \", total_rewards)\n",
    "    print(f\"Average Reward over {episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Flappy Bird Screen env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent, episode_rewards = train_monte_carlo_agent('TextFlappyBird-screen-v0', episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained Monte Carlo agent\n",
    "test_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Flappy Bird env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent, episode_rewards = train_monte_carlo_agent('TextFlappyBird-v0', episodes=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have set the reward treshold to 10000 in my function so the maximum reward that one episode can reach is 10000 and the maximum average reward over 100 episodes that we can reach is 10000 should not exceed 10000.\n",
    "\n",
    "If we go in this case that mean that the model learn extremly well and can go and have a very high score. We need to stop the test other wise it will take infinite amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the two different environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can saw from the first implementation, with the \"screen\" environment the Monte Carlo model is not able to converge at all to a correct policy.\n",
    "Also the computation time way more longer.\n",
    "\n",
    "However with the classical env that returns the distance of the player from the center of the closest upcoming pipe gap we manage to converge quite quickly and also to have a very optimal policy.\n",
    "\n",
    "In global, for this agent the classical env return more exploitable feature in order to converge.\n",
    "\n",
    "Therefor all the following analysis will be conducted on the classical environment, as this is the more pertinent for the Monte Carlo based agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to parameters (exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.8, 0.85, 0.9, 0.95, 1.0]  # Gamma values\n",
    "epsilons = [0.1, 0.2, 0.3, 0.4, 0.5]  # Epsilon values\n",
    "performance_metrics = np.zeros((len(gammas), len(epsilons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gamma in enumerate(gammas):\n",
    "    for j, epsilon in enumerate(epsilons):\n",
    "        trained_agent = train_monte_carlo_agent('TextFlappyBird-v0', episodes=20000, gamma=gamma, epsilon=epsilon)\n",
    "        total_reward = test_agent(trained_agent, episodes=100)\n",
    "        performance_metrics[i, j] = total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(performance_metrics, xticklabels=epsilons, yticklabels=gammas, annot=True, cmap=\"YlGnBu\")\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Gamma')\n",
    "plt.title('Agent Performance for Different Gamma and Epsilon Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This heatmap displays the agent's performance across a range of discount factors (gamma) and exploration rates (epsilon). Here are some key takeaways:\n",
    "\n",
    "- Gamma Impact: Higher gamma values seem to yield better performance in some cases, suggesting a preference for long-term rewards.\n",
    "- Epsilon Variability: The agent's performance does not consistently improve or worsen with higher epsilon values, indicating that the optimal exploration rate is context-dependent.\n",
    "- Parameter Sensitivity: The agent shows varied sensitivity to different gamma and epsilon combinations, with no single trend dominating, highlighting the importance of parameter tuning.\n",
    "- Performance Pockets: There are pockets where the agent performs particularly well, which could guide the fine-tuning of gamma and epsilon for this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence / Reward analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent and get reward history\n",
    "trained_agent, reward_history = train_monte_carlo_agent('TextFlappyBird-v0', episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reward history\n",
    "plt.plot(reward_history)\n",
    "plt.title('Reward History Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final score analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../agents/mc-agent.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge final score that hit the fixed treshold.\n",
    "\n",
    "Demonstrate a huge capability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test of trained agent on different level configuration (check overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_monte_carlo_agent(env_str, configs, episodes=1000, gamma=0.9, epsilon=0.1):\n",
    "    results = {}\n",
    "\n",
    "    for config in configs:\n",
    "        # Unpack the configuration dictionary to environment parameters\n",
    "        env = gym.make(env_str, **config)\n",
    "        agent = OffPolicyMonteCarloAgent(env, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "        episode_rewards = []  # List to store total reward from each episode\n",
    "\n",
    "        for _ in tqdm(range(episodes), desc=f\"Training with config: {config}\"):\n",
    "            # Create the behavior policy from current Q\n",
    "            behavior_policy = agent.create_behavior_policy(agent.Q)\n",
    "            # Generate an episode using the behavior policy\n",
    "            episode = agent.generate_episode(behavior_policy)\n",
    "            # Update Q-values based on the episode\n",
    "            agent.update_Q(episode)\n",
    "\n",
    "            # Calculate total reward for the episode and store it\n",
    "            total_reward = sum([reward for (_, _, reward) in episode])\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "        print(f\"Training completed for config: {config}.\")\n",
    "        results[str(config)] = episode_rewards  # Store the rewards with the config as the key\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with config: {'height': 15, 'width': 20, 'pipe_gap': 4}: 100%|ââââââââââ| 10000/10000 [00:52<00:00, 189.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for config: {'height': 15, 'width': 20, 'pipe_gap': 4}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with config: {'height': 20, 'width': 25, 'pipe_gap': 7}: 100%|ââââââââââ| 10000/10000 [01:38<00:00, 101.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for config: {'height': 20, 'width': 25, 'pipe_gap': 7}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with config: {'height': 25, 'width': 33, 'pipe_gap': 10}: 100%|ââââââââââ| 10000/10000 [02:01<00:00, 82.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for config: {'height': 25, 'width': 33, 'pipe_gap': 10}.\n"
     ]
    }
   ],
   "source": [
    "# Define a list of different configurations\n",
    "configurations = [\n",
    "    {'height': 15, 'width': 20, 'pipe_gap': 4},\n",
    "    {'height': 20, 'width': 25, 'pipe_gap': 7},\n",
    "    {'height': 25, 'width': 33, 'pipe_gap': 10}\n",
    "]\n",
    "\n",
    "# Train and test the agent over different configurations\n",
    "results = train_monte_carlo_agent('TextFlappyBird-v0', configurations, episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617467\n",
      "1380764\n",
      "1110167\n"
     ]
    }
   ],
   "source": [
    "for config in results:\n",
    "    print(\"Total reward over the new config:\", sum(results[config]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is doing very well even with new configuration of the env.\n",
    "\n",
    "That mean that it manage to learn the dynamics of the game, and don't overfit to the config."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLambdaAgent:\n",
    "    def __init__(self, env, gamma=0.9, lambda_=0.9, epsilon=0.1, alpha=0.5):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.E = defaultdict(lambda: np.zeros(env.action_space.n))  # Eligibility traces\n",
    "\n",
    "    def zero_action_value(self):\n",
    "        return np.zeros(self.env.action_space.n)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        delta = reward + self.gamma * self.Q[next_state][next_action] * (not done) - self.Q[state][action]\n",
    "        self.E[state][action] += 1  # Increment eligibility trace\n",
    "\n",
    "        for s, values in self.Q.items():\n",
    "            for a in range(len(values)):\n",
    "                self.Q[s][a] += self.alpha * delta * self.E[s][a]\n",
    "                self.E[s][a] *= self.gamma * self.lambda_\n",
    "\n",
    "    def reset_eligibility_traces(self):\n",
    "        for s in self.E:\n",
    "            for a in range(len(self.E[s])):\n",
    "                self.E[s][a] = 0\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        for i_episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            action = self.choose_action(str(state))\n",
    "            self.reset_eligibility_traces()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                next_action = self.choose_action(str(next_state))\n",
    "                self.update(str(state), action, reward, str(next_state), next_action, done)\n",
    "                state = next_state\n",
    "                action = next_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EC-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
