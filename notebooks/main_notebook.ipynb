{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Flappy Bird\n",
    "*Romain Mondelice*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to apply reinforcement learning methods to a\n",
    "simple game called Text Flappy Bird (TFB). The game is a variation to the\n",
    "well know Flappy Bird in which the player is made with a simple unit-element\n",
    "character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import text_flappy_bird_gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate environment\n",
    "env = gym.make('TextFlappyBird-v0', height = 15, width = 20, pipe_gap = 4)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "while True:\n",
    "        # Select next action\n",
    "        action = env.action_space.sample()  # for an agent, action = agent.policy(observation)\n",
    "\n",
    "        # Appy action and return new observation of the environment\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Render the game\n",
    "        os.system(\"clear\")\n",
    "        sys.stdout.write(env.render())\n",
    "        time.sleep(0.2) # FPS\n",
    "\n",
    "        # If player is dead break\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffPolicyMonteCarloAgent:\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(self.zero_action_value)\n",
    "        self.C = defaultdict(self.zero_action_value)\n",
    "        self.target_policy = defaultdict(int)\n",
    "\n",
    "    def zero_action_value(self):\n",
    "        \"\"\"Returns a default value for actions, a zero array with the size of the action space.\"\"\"\n",
    "        return np.zeros(self.env.action_space.n)\n",
    "        \n",
    "    def generate_episode(self, policy):\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Convert state to a string representation.\n",
    "            str_state = str(state)\n",
    "\n",
    "            if str_state in policy:\n",
    "                action_probs = policy[str_state]\n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            else:\n",
    "                # Fallback if the state is not in the policy, use uniform random selection\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _, info = self.env.step(action)\n",
    "            episode.append((str_state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def get_probs(self, Q_s, epsilon, nA):\n",
    "        \"\"\"Obtains the policy for a given state\"\"\"\n",
    "        policy_s = np.ones(nA) * epsilon / nA\n",
    "        best_a = np.argmax(Q_s)\n",
    "        policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "        return policy_s\n",
    "    \n",
    "    def update_Q(self, episode):\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = self.gamma * G + reward\n",
    "            self.C[state][action] += W\n",
    "            self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\n",
    "            self.target_policy[state] = np.argmax(self.Q[state])\n",
    "            \n",
    "            if action != self.target_policy[state]:\n",
    "                break\n",
    "            W = W * 1./self.get_probs(self.Q[state], self.epsilon, self.env.action_space.n)[action]\n",
    "            \n",
    "    def train(self, num_episodes):\n",
    "        for i_episode in range(1, num_episodes + 1):\n",
    "            episode = self.generate_episode(policy=self.create_behavior_policy(self.Q))\n",
    "            self.update_Q(episode)\n",
    "    \n",
    "    def create_behavior_policy(self, Q):\n",
    "        \"\"\"Creates a behavior policy using Îµ-greedy approach based on Q.\"\"\"\n",
    "        behavior_policy = {}\n",
    "        for state, actions in Q.items():\n",
    "            behavior_policy[state] = self.get_probs(actions, self.epsilon, self.env.action_space.n)\n",
    "        return behavior_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_monte_carlo_agent(env_str, episodes=1000, gamma=0.9, epsilon=0.1):\n",
    "    env = gym.make(env_str, height=15, width=20, pipe_gap=4)\n",
    "    agent = OffPolicyMonteCarloAgent(env, gamma=gamma, epsilon=epsilon)\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc=\"Training process\"):\n",
    "        # Create the behavior policy from current Q\n",
    "        behavior_policy = agent.create_behavior_policy(agent.Q)\n",
    "        # Generate an episode using the behavior policy\n",
    "        episode = agent.generate_episode(behavior_policy)\n",
    "        # Update Q-values based on the episode\n",
    "        agent.update_Q(episode)\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, episodes=100, reward_threshold=10000):\n",
    "    total_rewards = 0\n",
    "    episode_scores = []  # To store the score of each episode\n",
    "\n",
    "    for episode_num in tqdm(range(1, episodes + 1), desc=\"Testing episodes\"):\n",
    "        state = agent.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Convert state to a string representation for consistency.\n",
    "            str_state = str(state)\n",
    "            \n",
    "            # Use the target_policy for action selection if this state has been seen.\n",
    "            # Otherwise, select a random action.\n",
    "            if str_state in agent.target_policy:\n",
    "                action = agent.target_policy[str_state]\n",
    "            else:\n",
    "                action = agent.env.action_space.sample()\n",
    "\n",
    "            state, reward, done, _, info = agent.env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Check if the reward threshold for this episode has been exceeded\n",
    "            if episode_reward > reward_threshold:\n",
    "                break\n",
    "\n",
    "        # Episode is done or threshold exceeded, append its total reward to episode_scores\n",
    "        episode_scores.append(episode_reward)\n",
    "        total_rewards += episode_reward\n",
    "    \n",
    "    avg_reward = total_rewards / episodes\n",
    "    print(\"Total reward across all episodes: \", total_rewards)\n",
    "    print(f\"Average Reward over {episodes} episodes: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Flappy Bird Screen env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent = train_monte_carlo_agent('TextFlappyBird-screen-v0', episodes=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained Monte Carlo agent\n",
    "test_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `trained_agent` is your Monte Carlo agent that you have trained\n",
    "with open('../agents/mc-agent-screen.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Flappy Bird env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_agent = train_monte_carlo_agent('TextFlappyBird-v0', episodes=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have set the reward treshold to 10000 in my function so the maximum reward that one episode can reach is 10000 and the maximum average reward over 100 episodes that we can reach is 10000 should not exceed 10000.\n",
    "\n",
    "If we go in this case that mean that the model learn extremly well and can go and have a very high score. We need to stop the test other wise it will take infinite amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing episodes: 100%|ââââââââââ| 100/100 [00:08<00:00, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward across all episodes:  1000100\n",
      "Average Reward over 100 episodes: 10001.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `trained_agent` is your Monte Carlo agent that you have trained\n",
    "with open('../agents/mc-agent.pkl', 'wb') as f:\n",
    "    pickle.dump(trained_agent, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa based agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaLambdaAgent:\n",
    "    def __init__(self, env, lambda_):\n",
    "        self.env = env\n",
    "        self.lambda_ = lambda_\n",
    "        # Initialize Q-values and eligibility traces\n",
    "        # Define epsilon for epsilon-greedy policy\n",
    "    \n",
    "    def policy(self, observation):\n",
    "        # Define epsilon-greedy policy here\n",
    "        return action\n",
    "    \n",
    "    def update_q_values(self, state, action, reward, next_state, next_action):\n",
    "        # Update Q-values using the Sarsa(Î») formula\n",
    "    \n",
    "    def update_eligibility_traces(self, state, action):\n",
    "        # Update eligibility traces\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EC-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
